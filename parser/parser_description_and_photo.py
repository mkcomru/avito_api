import requests
import cloudscraper
import httpx
import subprocess
import json
import time
import random
import re
from urllib.parse import urlparse, urljoin
from fake_useragent import UserAgent
from bs4 import BeautifulSoup
from dataclasses import dataclass
from typing import Optional, List, Dict, Any
import threading
import queue
import os
import tempfile
import sys

@dataclass
class ParseResult:
    url: str
    description: Optional[str] = None
    images: List[str] = None
    method: str = "unknown"
    success: bool = False
    error: Optional[str] = None
    response_time: float = 0.0
    
    def __post_init__(self):
        if self.images is None:
            self.images = []

class AdvancedMultiMethodParser:
    def __init__(self):
        print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ü–†–û–î–í–ò–ù–£–¢–´–ô –ú–£–õ–¨–¢–ò–ú–ï–¢–û–î–ù–´–ô Avito Parser...")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∑–∞–¥–µ—Ä–∂–µ–∫
        self.delay_config = {
            'min_delay': 3,        # 3 —Å–µ–∫—É–Ω–¥—ã –º–∏–Ω–∏–º—É–º
            'max_delay': 15,       # 15 —Å–µ–∫—É–Ω–¥ –º–∞–∫—Å–∏–º—É–º
            'between_methods': 2   # 2 —Å–µ–∫—É–Ω–¥—ã –º–µ–∂–¥—É –º–µ—Ç–æ–¥–∞–º–∏
        }
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –º–µ—Ç–æ–¥–∞–º
        self.method_stats = {
            'requests_stealth': {'success': 0, 'total': 0},
            'cloudscraper_advanced': {'success': 0, 'total': 0},
            'httpx_stealth': {'success': 0, 'total': 0},
            'curl_stealth': {'success': 0, 'total': 0},
            'requests_mobile': {'success': 0, 'total': 0},
            'requests_api': {'success': 0, 'total': 0}
        }
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self._init_user_agents()
        self._init_requests_session()
        self._init_cloudscraper()
        self._init_httpx_client()
        
        self.last_request_time = 0
        
        print("‚úÖ –ü–†–û–î–í–ò–ù–£–¢–´–ô –ú–£–õ–¨–¢–ò–ú–ï–¢–û–î–ù–´–ô Parser –≥–æ—Ç–æ–≤!")
    
    def _init_user_agents(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∞–º—ã—Ö —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö User-Agent'–æ–≤"""
        # –°–ê–ú–´–ï –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ UA –≤ –†–æ—Å—Å–∏–∏ (—è–Ω–≤–∞—Ä—å 2025)
        self.premium_desktop_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
        ]
        
        self.premium_mobile_agents = [
            'Mozilla/5.0 (iPhone; CPU iPhone OS 17_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Mobile/15E148 Safari/604.1',
            'Mozilla/5.0 (Linux; Android 14; SM-G998B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Mobile Safari/537.36',
            'Mozilla/5.0 (Linux; Android 13; Pixel 7 Pro) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Mobile Safari/537.36'
        ]
        
        try:
            self.ua = UserAgent()
            print("‚úÖ UserAgent –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
        except Exception:
            self.ua = None
    
    def _get_premium_desktop_ua(self):
        """–ü–æ–ª—É—á–∞–µ—Ç –ø—Ä–µ–º–∏—É–º –¥–µ—Å–∫—Ç–æ–ø–Ω—ã–π UA"""
        return random.choice(self.premium_desktop_agents)
    
    def _get_premium_mobile_ua(self):
        """–ü–æ–ª—É—á–∞–µ—Ç –ø—Ä–µ–º–∏—É–º –º–æ–±–∏–ª—å–Ω—ã–π UA"""
        return random.choice(self.premium_mobile_agents)
    
    def _init_requests_session(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π Requests —Å–µ—Å—Å–∏–∏"""
        self.requests_session = requests.Session()
        
        # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
        self.stealth_headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'ru-RU,ru;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
            'sec-ch-ua': '"Not_A Brand";v="8", "Chromium";v="121", "Google Chrome";v="121"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"'
        }
        
        print("‚úÖ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è Requests —Å–µ—Å—Å–∏—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
    
    def _init_cloudscraper(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ CloudScraper"""
        try:
            # –ë–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
            self.scraper = cloudscraper.create_scraper(
                browser={
                    'browser': 'chrome',
                    'platform': 'windows',
                    'desktop': True,
                    'mobile': False
                },
                delay=random.uniform(1, 5),
                debug=False,
                requestPostHook=self._cloudscraper_post_hook,
                requestPreHook=self._cloudscraper_pre_hook
            )
            print("‚úÖ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π CloudScraper –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ CloudScraper: {e}")
            self.scraper = None
    
    def _cloudscraper_pre_hook(self, request, **kwargs):
        """–•—É–∫ –ø–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ—Å–æ–º CloudScraper (–ò–°–ü–†–ê–í–õ–ï–ù–û)"""
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
        request.headers.update({
            'User-Agent': self._get_premium_desktop_ua(),
            'Referer': 'https://www.google.com/',
            'Origin': 'https://www.avito.ru'
        })
        return request

    def _cloudscraper_post_hook(self, response, **kwargs):
        """–•—É–∫ –ø–æ—Å–ª–µ –æ—Ç–≤–µ—Ç–∞ CloudScraper (–ò–°–ü–†–ê–í–õ–ï–ù–û)"""
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –ø–æ—Å–ª–µ –∑–∞–ø—Ä–æ—Å–∞
        time.sleep(random.uniform(0.5, 2.0))
        return response

    def _init_httpx_client(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ HTTPX –∫–ª–∏–µ–Ω—Ç–∞ (–ò–°–ü–†–ê–í–õ–ï–ù–û)"""
        try:
            # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è HTTPX –ë–ï–ó HTTP/2
            self.httpx_client = httpx.Client(
                timeout=httpx.Timeout(30.0, connect=10.0),
                follow_redirects=True,
                verify=True,
                # http2=True,  # –£–ë–ò–†–ê–ï–ú HTTP/2 –ø–æ–∫–∞ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–∏–º h2
                limits=httpx.Limits(max_keepalive_connections=5, max_connections=10)
            )
            print("‚úÖ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π HTTPX –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ HTTPX: {e}")
            self.httpx_client = None
    
    def _wait_between_requests(self):
        """–£–º–Ω–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        required_delay = random.uniform(
            self.delay_config['min_delay'], 
            self.delay_config['max_delay']
        )
        
        if time_since_last < required_delay:
            sleep_time = required_delay - time_since_last
            print(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ {sleep_time:.1f} —Å–µ–∫—É–Ω–¥...")
            time.sleep(sleep_time)
        
        self.last_request_time = time.time()
    
    def _get_realistic_referer(self):
        """–ü–æ–ª—É—á–∞–µ—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π referer"""
        referers = [
            'https://www.google.com/search?q=–∞–≤–∏—Ç–æ+–¥–µ—Ç—Å–∫–∏–µ+—Ç–æ–≤–∞—Ä—ã+–º–æ—Å–∫–≤–∞',
            'https://yandex.ru/search/?text=–ø–æ–¥—É—à–∫–∞+–∫–æ—à–∞—á—å—è+–ª–∞–ø–∞+–∞–≤–∏—Ç–æ',
            'https://www.avito.ru/',
            'https://www.avito.ru/moskva',
            'https://www.avito.ru/moskva/tovary_dlya_detey_i_igrushki',
            'https://dzen.ru/',
            'https://vk.com/'
        ]
        return random.choice(referers)
    
    def parse_url(self, url: str) -> ParseResult:
        """–ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤"""
        print(f"\nüöÄ –ü–†–û–î–í–ò–ù–£–¢–´–ô –ú–£–õ–¨–¢–ò–ú–ï–¢–û–î–ù–´–ô –ø–∞—Ä—Å–∏–Ω–≥: {url}")
        
        if not self._is_avito_url(url):
            return ParseResult(url=url, error="–ù–µ–≤–µ—Ä–Ω—ã–π URL –ê–≤–∏—Ç–æ", success=False)
        
        self._wait_between_requests()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Ä—è–¥–æ–∫ –º–µ—Ç–æ–¥–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        methods = self._get_optimal_method_order()
        
        for i, method in enumerate(methods):
            print(f"\nüéØ –ú–µ—Ç–æ–¥ {i+1}/{len(methods)}: {method}")
            
            start_time = time.time()
            result = self._try_method(method, url)
            result.response_time = time.time() - start_time
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self._update_method_stats(method, result.success)
            
            if result.success:
                print(f"‚úÖ –£–°–ü–ï–• —á–µ—Ä–µ–∑ {method}!")
                return result
            else:
                print(f"‚ùå {method} –Ω–µ—É–¥–∞—á–µ–Ω: {result.error}")
                if i < len(methods) - 1:  # –ï—Å–ª–∏ –Ω–µ –ø–æ—Å–ª–µ–¥–Ω–∏–π –º–µ—Ç–æ–¥
                    print(f"‚è≥ –ü–∞—É–∑–∞ {self.delay_config['between_methods']}—Å –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º –º–µ—Ç–æ–¥–æ–º...")
                    time.sleep(self.delay_config['between_methods'])
        
        return ParseResult(
            url=url, 
            error="–í—Å–µ –º–µ—Ç–æ–¥—ã –Ω–µ—É–¥–∞—á–Ω—ã", 
            success=False,
            method="all_failed"
        )
    
    def _get_optimal_method_order(self):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ –º–µ—Ç–æ–¥–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        methods_with_rates = []
        
        for method, stats in self.method_stats.items():
            if stats['total'] > 0:
                success_rate = stats['success'] / stats['total']
            else:
                success_rate = 0.5  # –°—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –Ω–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤
            
            methods_with_rates.append((method, success_rate))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
        methods_with_rates.sort(key=lambda x: x[1], reverse=True)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–∞–∑–≤–∞–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤
        return [method for method, rate in methods_with_rates]
    
    def _try_method(self, method: str, url: str) -> ParseResult:
        """–ü—Ä–æ–±—É–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        try:
            if method == 'requests_stealth':
                return self._try_requests_stealth_parsing(url)
            elif method == 'cloudscraper_advanced':
                return self._try_cloudscraper_advanced_parsing(url)
            elif method == 'httpx_stealth':
                return self._try_httpx_stealth_parsing(url)
            elif method == 'curl_stealth':
                return self._try_curl_stealth_parsing(url)
            elif method == 'requests_mobile':
                return self._try_requests_mobile_parsing(url)
            elif method == 'requests_api':
                return self._try_requests_api_parsing(url)
            else:
                return ParseResult(url=url, method=method, error="–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥", success=False)
                
        except Exception as e:
            return ParseResult(url=url, method=method, error=str(e), success=False)
    
    def _try_requests_stealth_parsing(self, url: str) -> ParseResult:
        """–°—Ç–µ–ª—Å-–ø–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ —É–ª—É—á—à–µ–Ω–Ω—ã–π Requests"""
        try:
            headers = self.stealth_headers.copy()
            headers['User-Agent'] = self._get_premium_desktop_ua()
            headers['Referer'] = self._get_realistic_referer()
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏
            if random.choice([True, False]):
                headers['X-Requested-With'] = 'XMLHttpRequest'
            
            session = requests.Session()
            session.headers.update(headers)
            
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É
            response = session.get(url, timeout=30)
            response.encoding = 'utf-8'  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º UTF-8
            
            print(f"üìä Requests Stealth –æ—Ç–≤–µ—Ç: {response.status_code}")
            
            if response.status_code == 200:
                if self._detect_blocking(response.text, response.headers):
                    return ParseResult(url=url, method="requests_stealth", error="–ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∞", success=False)
                
                description, images = self._extract_content_from_html(response.content)
                
                if description or images:
                    return ParseResult(
                        url=url,
                        description=description,
                        images=images,
                        method="requests_stealth",
                        success=True
                    )
            
            return ParseResult(url=url, method="requests_stealth", error=f"HTTP {response.status_code}", success=False)
            
        except UnicodeError as e:
            return ParseResult(url=url, method="requests_stealth", error=f"–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏: {e}", success=False)
        except Exception as e:
            return ParseResult(url=url, method="requests_stealth", error=str(e), success=False)
    
    def _try_cloudscraper_advanced_parsing(self, url: str) -> ParseResult:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ CloudScraper"""
        if not self.scraper:
            return ParseResult(url=url, method="cloudscraper_advanced", error="CloudScraper –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω", success=False)
        
        try:
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –æ–±—Ö–æ–¥–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏
            extra_headers = {
                'User-Agent': self._get_premium_desktop_ua(),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                'Accept-Language': 'ru-RU,ru;q=0.9',
                'Accept-Encoding': 'gzip, deflate, br',
                'Referer': self._get_realistic_referer(),
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Cache-Control': 'max-age=0'
            }
            
            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ—Å–æ–º
            time.sleep(random.uniform(1, 3))
            
            response = self.scraper.get(url, headers=extra_headers, timeout=45)
            response.encoding = 'utf-8'  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º UTF-8
            
            print(f"üìä CloudScraper Advanced –æ—Ç–≤–µ—Ç: {response.status_code}")
            
            if response.status_code == 200:
                if self._detect_blocking(response.text, response.headers):
                    return ParseResult(url=url, method="cloudscraper_advanced", error="–ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∞", success=False)
                
                description, images = self._extract_content_from_html(response.content)
                
                if description or images:
                    return ParseResult(
                        url=url,
                        description=description,
                        images=images,
                        method="cloudscraper_advanced",
                        success=True
                    )
            
            return ParseResult(url=url, method="cloudscraper_advanced", error=f"HTTP {response.status_code}", success=False)
            
        except Exception as e:
            return ParseResult(url=url, method="cloudscraper_advanced", error=str(e), success=False)
    
    def _try_httpx_stealth_parsing(self, url: str) -> ParseResult:
        """–°—Ç–µ–ª—Å-–ø–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ —É–ª—É—á—à–µ–Ω–Ω—ã–π HTTPX"""
        if not self.httpx_client:
            return ParseResult(url=url, method="httpx_stealth", error="HTTPX –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω", success=False)
        
        try:
            headers = {
                'User-Agent': self._get_premium_desktop_ua(),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                'Accept-Language': 'ru-RU,ru;q=0.9',
                'Accept-Encoding': 'gzip, deflate, br',
                'Referer': self._get_realistic_referer(),
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Cache-Control': 'max-age=0',
                'sec-ch-ua': '"Not_A Brand";v="8", "Chromium";v="121", "Google Chrome";v="121"',
                'sec-ch-ua-mobile': '?0',
                'sec-ch-ua-platform': '"Windows"'
            }
            
            response = self.httpx_client.get(url, headers=headers)
            
            print(f"üìä HTTPX Stealth –æ—Ç–≤–µ—Ç: {response.status_code}")
            
            if response.status_code == 200:
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É
                content = response.content
                text = response.text
                
                if self._detect_blocking(text, dict(response.headers)):
                    return ParseResult(url=url, method="httpx_stealth", error="–ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∞", success=False)
                
                description, images = self._extract_content_from_html(content)
                
                if description or images:
                    return ParseResult(
                        url=url,
                        description=description,
                        images=images,
                        method="httpx_stealth",
                        success=True
                    )
            
            return ParseResult(url=url, method="httpx_stealth", error=f"HTTP {response.status_code}", success=False)
            
        except Exception as e:
            return ParseResult(url=url, method="httpx_stealth", error=str(e), success=False)
    
    def _try_curl_stealth_parsing(self, url: str) -> ParseResult:
        """–°—Ç–µ–ª—Å-–ø–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ —É–ª—É—á—à–µ–Ω–Ω—ã–π CURL (–ò–°–ü–†–ê–í–õ–ï–ù–û)"""
        try:
            headers = [
                f"User-Agent: {self._get_premium_desktop_ua()}",
                "Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
                "Accept-Language: ru-RU,ru;q=0.9",
                "Accept-Encoding: gzip, deflate, br",
                f"Referer: {self._get_realistic_referer()}",
                "DNT: 1",
                "Connection: keep-alive",
                "Upgrade-Insecure-Requests: 1",
                "Cache-Control: max-age=0"
            ]
            
            cmd = [
                'curl',
                '-s',  # silent
                '-L',  # follow redirects
                '--compressed',  # accept gzip
                '--max-time', '30',
                '--connect-timeout', '10',
                # '--http2',  # –£–ë–ò–†–ê–ï–ú HTTP/2
                '--retry', '2',
                '--retry-delay', '1'
            ]
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
            for header in headers:
                cmd.extend(['-H', header])
            
            cmd.append(url)
            
            print("üîß –í—ã–ø–æ–ª–Ω—è–µ–º CURL Stealth –∑–∞–ø—Ä–æ—Å...")
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É –æ–∫—Ä—É–∂–µ–Ω–∏—è
            env = os.environ.copy()
            env['LC_ALL'] = 'en_US.UTF-8'
            
            result = subprocess.run(
                cmd, 
                capture_output=True, 
                text=True, 
                timeout=45,
                encoding='utf-8',
                errors='replace',  # –ó–∞–º–µ–Ω—è–µ–º –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
                env=env
            )
            
            if result.returncode == 0 and result.stdout:
                print(f"üìä CURL Stealth —É—Å–ø–µ—à–µ–Ω: {len(result.stdout)} —Å–∏–º–≤–æ–ª–æ–≤")
                
                if self._detect_blocking(result.stdout):
                    return ParseResult(url=url, method="curl_stealth", error="–ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∞", success=False)
                
                description, images = self._extract_content_from_html(result.stdout.encode('utf-8', errors='replace'))
                
                if description or images:
                    return ParseResult(
                        url=url,
                        description=description,
                        images=images,
                        method="curl_stealth",
                        success=True
                    )
        
        except subprocess.TimeoutExpired:
            return ParseResult(url=url, method="curl_stealth", error="CURL timeout", success=False)
        except FileNotFoundError:
            return ParseResult(url=url, method="curl_stealth", error="CURL –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω", success=False)
        except Exception as e:
            return ParseResult(url=url, method="curl_stealth", error=str(e), success=False)
    
    def _try_requests_mobile_parsing(self, url: str) -> ParseResult:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –º–æ–±–∏–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–±–∏–ª—å–Ω—É—é –≤–µ—Ä—Å–∏—é —Å–∞–π—Ç–∞
            mobile_url = url.replace('www.avito.ru', 'm.avito.ru')
            
            # –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –º–æ–±–∏–ª—å–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
            mobile_headers = {
                'User-Agent': self._get_premium_mobile_ua(),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ru-RU,ru;q=0.9',
                'Accept-Encoding': 'gzip, deflate, br',
                'Referer': 'https://www.google.com/',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Cache-Control': 'max-age=0',
                'Viewport-Width': '375',
                'Device-Memory': '4',
                'Downlink': '10'
            }
            
            mobile_session = requests.Session()
            mobile_session.headers.update(mobile_headers)
            
            print(f"üì± –ü—Ä–æ–±—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é –º–æ–±–∏–ª—å–Ω—É—é –≤–µ—Ä—Å–∏—é: {mobile_url[:50]}...")
            
            response = mobile_session.get(mobile_url, timeout=30)
            response.encoding = 'utf-8'  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º UTF-8
            
            print(f"üìä Mobile Requests –æ—Ç–≤–µ—Ç: {response.status_code}")
            
            if response.status_code == 200:
                if self._detect_blocking(response.text, response.headers):
                    return ParseResult(url=url, method="requests_mobile", error="–ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∞", success=False)
                
                description, images = self._extract_content_from_html(response.content, is_mobile=True)
                
                if description or images:
                    return ParseResult(
                        url=url,
                        description=description,
                        images=images,
                        method="requests_mobile",
                        success=True
                    )
            
            return ParseResult(url=url, method="requests_mobile", error=f"HTTP {response.status_code}", success=False)
            
        except Exception as e:
            return ParseResult(url=url, method="requests_mobile", error=str(e), success=False)
    
    def _try_requests_api_parsing(self, url: str) -> ParseResult:
        """–ü–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ API —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã"""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏–∑ URL
            item_id = self._extract_item_id(url)
            if not item_id:
                return ParseResult(url=url, method="requests_api", error="–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å ID", success=False)
            
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ API —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã
            api_urls = [
                f"https://m.avito.ru/api/1/items/{item_id}",
                f"https://www.avito.ru/web/1/items/{item_id}",
                f"https://www.avito.ru/api/1/items/{item_id}/extended"
            ]
            
            headers = {
                'User-Agent': self._get_premium_mobile_ua(),
                'Accept': 'application/json, text/plain, */*',
                'Accept-Language': 'ru-RU,ru;q=0.9',
                'Referer': url,
                'X-Requested-With': 'XMLHttpRequest'
            }
            
            for api_url in api_urls:
                try:
                    print(f"üîç –ü—Ä–æ–±—É–µ–º API: {api_url}")
                    
                    session = requests.Session()
                    response = session.get(api_url, headers=headers, timeout=20)
                    response.encoding = 'utf-8'
                    
                    if response.status_code == 200:
                        try:
                            data = response.json()
                            description, images = self._extract_from_api_response(data)
                            
                            if description or images:
                                return ParseResult(
                                    url=url,
                                    description=description,
                                    images=images,
                                    method="requests_api",
                                    success=True
                                )
                        except json.JSONDecodeError:
                            continue
                            
                except Exception as e:
                    print(f"‚ö†Ô∏è API –æ—à–∏–±–∫–∞ {api_url}: {e}")
                    continue
            
            return ParseResult(url=url, method="requests_api", error="API —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã", success=False)
            
        except Exception as e:
            return ParseResult(url=url, method="requests_api", error=str(e), success=False)
    
    def _extract_item_id(self, url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç ID –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏–∑ URL"""
        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è ID –∏–∑ URL –ê–≤–∏—Ç–æ
        pattern = r'_(\d+)$'
        match = re.search(pattern, url)
        if match:
            return match.group(1)
        return None
    
    def _extract_from_api_response(self, data: dict):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ API –æ—Ç–≤–µ—Ç–∞"""
        description = None
        images = []
        
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–ª—é—á–∏ –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏—è
        desc_keys = ['description', 'text', 'body', 'content', 'details']
        for key in desc_keys:
            if key in data and data[key]:
                description = str(data[key]).strip()
                if len(description) > 10:
                    break
        
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–ª—é—á–∏ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        img_keys = ['images', 'photos', 'pictures', 'gallery']
        for key in img_keys:
            if key in data and isinstance(data[key], list):
                for img in data[key]:
                    if isinstance(img, dict):
                        url = img.get('url') or img.get('src') or img.get('link')
                        if url:
                            images.append(url)
                    elif isinstance(img, str):
                        images.append(img)
        
        return description, images
    
    def _detect_blocking(self, text: str, headers: dict = None) -> bool:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –¥–µ—Ç–µ–∫—Ü–∏—è –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫"""
        if not text:
            return True
            
        text_lower = text.lower()
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
        blocking_indicators = [
            'captcha', 'cloudflare', 'access denied', '–¥–æ—Å—Ç—É–ø –æ–≥—Ä–∞–Ω–∏—á–µ–Ω',
            '—Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤', 'too many requests', 'robot check',
            '–ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å', 'suspicious activity',
            'security check', '–ø—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏',
            'temporarily blocked', '–≤—Ä–µ–º–µ–Ω–Ω–∞—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞',
            'rate limit', 'forbidden', '403', '429',
            'blocked', 'challenge', 'verification',
            '–∞–≤–∏—Ç–æ –∑–∞—â–∏—Ç–∞', 'avito protection',
            '–Ω–∞—Ä—É—à–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª', 'violation of rules',
            '–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å', 'automatic activity',
            '–ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ', 'check your connection',
            '–ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—É', 'reload the page'
        ]
        
        for indicator in blocking_indicators:
            if indicator in text_lower:
                print(f"üö® –ë–õ–û–ö–ò–†–û–í–ö–ê: {indicator}")
                return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É –æ—Ç–≤–µ—Ç–∞
        if len(text) < 2000:
            print(f"üö® –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ê–≤–∏—Ç–æ
        essential_elements = ['avito', 'item']
        found_elements = sum(1 for element in essential_elements if element in text_lower)
        
        if found_elements < 1:
            print(f"üö® –ù–µ –Ω–∞–π–¥–µ–Ω—ã –∫–ª—é—á–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã: {found_elements}/{len(essential_elements)}")
            return True
        
        return False
    
    def _extract_content_from_html(self, html_content, is_mobile: bool = False):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ HTML"""
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É
            if isinstance(html_content, bytes):
                # –ü—Ä–æ–±—É–µ–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–¥–∏—Ä–æ–≤–∫—É –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
                from chardet import chardet
                try:
                    detected = chardet.detect(html_content)
                    encoding = detected.get('encoding', 'utf-8')
                    if encoding and encoding.lower() != 'utf-8':
                        html_content = html_content.decode(encoding, errors='replace').encode('utf-8')
                except:
                    # –ï—Å–ª–∏ chardet –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏–ª–∏ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
                    html_content = html_content.decode('utf-8', errors='replace').encode('utf-8')
            
            soup = BeautifulSoup(html_content, 'html.parser')
            
            description = self._extract_description_from_soup(soup, is_mobile)
            images = self._extract_images_from_soup(soup, is_mobile)
            
            return description, images
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {e}")
            return None, []
    
    def _extract_description_from_soup(self, soup, is_mobile: bool = False):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º–∏"""
        if is_mobile:
            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –º–æ–±–∏–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏
            mobile_selectors = [
                '.item-description-text',
                '[data-marker*="description"]',
                '.item-text',
                '.description',
                '.item-description',
                'meta[name="description"]',
                'meta[property="og:description"]',
                '.item-view-description',
                '.js-item-description',
                '[itemprop="description"]'
            ]
            selectors = mobile_selectors
        else:
            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –¥–µ—Å–∫—Ç–æ–ø–Ω–æ–π –≤–µ—Ä—Å–∏–∏
            desktop_selectors = [
                '[data-marker="item-view/item-description"]',
                '[data-marker*="description"]',
                '.item-description-text',
                '[itemprop="description"]',
                '.item-description',
                '.item-description-content',
                '.item-view-description',
                '.item-text',
                '.js-item-description',
                'meta[name="description"]',
                'meta[property="og:description"]',
                '.iva-item-text',
                '.description',
                '.text'
            ]
            selectors = desktop_selectors
        
        for selector in selectors:
            try:
                if selector.startswith('meta'):
                    element = soup.select_one(selector)
                    if element:
                        content = element.get('content', '').strip()
                        if len(content) > 20:
                            print(f"üìÑ –û–ø–∏—Å–∞–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–æ ({selector}): {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
                            return content
                else:
                    element = soup.select_one(selector)
                    if element:
                        text = element.get_text(separator=' ', strip=True)
                        if len(text) > 20:
                            print(f"üìÑ –û–ø–∏—Å–∞–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–æ ({selector}): {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
                            return text
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞ {selector}: {e}")
                continue
        
        print("‚ö†Ô∏è –û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        return None
    
    def _extract_images_from_soup(self, soup, is_mobile: bool = False):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º–∏"""
        images = []
        
        if is_mobile:
            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –º–æ–±–∏–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏
            mobile_selectors = [
                '.item-photo img',
                '.gallery img',
                'img[src*="avito"]',
                '.photo img',
                '.item-gallery img',
                '.js-gallery img'
            ]
            selectors = mobile_selectors
        else:
            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –¥–µ—Å–∫—Ç–æ–ø–Ω–æ–π –≤–µ—Ä—Å–∏–∏
            desktop_selectors = [
                '[data-marker="item-view/gallery"] img',
                '[data-marker*="gallery"] img',
                '.gallery-img-frame img',
                '.item-photo img',
                '.item-view img',
                'img[src*="avito-st.ru"]',
                'img[src*="avito.st"]',
                '.iva-item-photo img',
                '.gallery img',
                '.js-gallery img',
                '.item-gallery img'
            ]
            selectors = desktop_selectors
        
        for selector in selectors:
            try:
                img_elements = soup.select(selector)
                for img in img_elements:
                    src = img.get('src') or img.get('data-src') or img.get('data-lazy') or img.get('data-original')
                    if src and src not in images:
                        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º URL
                        if src.startswith('//'):
                            src = 'https:' + src
                        elif src.startswith('/'):
                            src = 'https://www.avito.ru' + src
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
                        if any(ext in src.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                            # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                            if not any(skip in src.lower() for skip in ['avatar', 'logo', 'icon', 'sprite', 'button', 'placeholder']):
                                images.append(src)
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π {selector}: {e}")
                continue
        
        print(f"üñºÔ∏è –ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(images)}")
        return images
    
    def _update_method_stats(self, method: str, success: bool):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –º–µ—Ç–æ–¥–æ–≤"""
        if method in self.method_stats:
            self.method_stats[method]['total'] += 1
            if success:
                self.method_stats[method]['success'] += 1
    
    def _is_avito_url(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç URL –ê–≤–∏—Ç–æ"""
        return 'avito.ru' in urlparse(url).netloc
    
    def close(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç –≤—Å–µ —Ä–µ—Å—É—Ä—Å—ã"""
        try:
            if hasattr(self, 'requests_session'):
                self.requests_session.close()
            
            if hasattr(self, 'httpx_client') and self.httpx_client:
                self.httpx_client.close()
            
            print("üîí –ü–†–û–î–í–ò–ù–£–¢–´–ô –ú–£–õ–¨–¢–ò–ú–ï–¢–û–î–ù–´–ô Parser –∑–∞–∫—Ä—ã—Ç")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–∫—Ä—ã—Ç–∏—è: {e}")
    
    def get_stats(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –º–µ—Ç–æ–¥–æ–≤"""
        stats = {}
        for method, data in self.method_stats.items():
            if data['total'] > 0:
                success_rate = (data['success'] / data['total']) * 100
                stats[method] = {
                    'success_rate': f"{success_rate:.1f}%",
                    'successful': data['success'],
                    'total': data['total']
                }
            else:
                stats[method] = {'success_rate': '0.0%', 'successful': 0, 'total': 0}
        
        return stats


def safe_parse_item(url: str) -> Dict[str, Any]:
    """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –ü–†–û–î–í–ò–ù–£–¢–´–ô –ú–£–õ–¨–¢–ò–ú–ï–¢–û–î–ù–´–ô –ø–∞—Ä—Å–∏–Ω–≥"""
    parser = None
    try:
        parser = AdvancedMultiMethodParser()
        result = parser.parse_url(url)
        
        return {
            'url': result.url,
            'description': result.description,
            'images': result.images,
            'method': result.method,
            'success': result.success,
            'error': result.error,
            'response_time': result.response_time,
            'stats': parser.get_stats()
        }
        
    except Exception as e:
        return {
            'url': url,
            'description': None,
            'images': [],
            'method': 'error',
            'success': False,
            'error': str(e),
            'response_time': 0.0,
            'stats': {}
        }
    finally:
        if parser:
            parser.close()


class ExtremeStealthAvitoParser:
    def __init__(self):
        print("ü•∑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≠–ö–°–¢–†–ï–ú–ê–õ–¨–ù–û –°–¢–ï–õ–° Avito Parser...")
        
        # –≠–ö–°–¢–†–ï–ú–ê–õ–¨–ù–´–ï –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–æ—Ç–∏–≤ 429
        self.extreme_delay_config = {
            'min_delay': 300,      # 5 –º–∏–Ω—É—Ç –º–∏–Ω–∏–º—É–º
            'max_delay': 1800,     # 30 –º–∏–Ω—É—Ç –º–∞–∫—Å–∏–º—É–º
            'between_methods': 60, # 1 –º–∏–Ω—É—Ç–∞ –º–µ–∂–¥—É –º–µ—Ç–æ–¥–∞–º–∏
            'exponential_base': 2,
            'max_exponential': 7200  # 2 —á–∞—Å–∞ –º–∞–∫—Å–∏–º—É–º
        }
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'blocked_requests': 0,
            'last_success_time': time.time(),
            'consecutive_failures': 0,
            'session_start_time': time.time()
        }
        
        self.last_request_time = 0
        self.session_proxies = []
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å–∞–º—ã–µ –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self._init_extreme_user_agents()
        
        print("‚úÖ –≠–ö–°–¢–†–ï–ú–ê–õ–¨–ù–û –°–¢–ï–õ–° Parser –≥–æ—Ç–æ–≤!")
    
    def _init_extreme_user_agents(self):
        """–¢–æ–ª—å–∫–æ —Å–∞–º—ã–µ –Ω–µ–∑–∞–º–µ—Ç–Ω—ã–µ User-Agent'—ã"""
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –±—Ä–∞—É–∑–µ—Ä–æ–≤ –≤ –†–æ—Å—Å–∏–∏ (—è–Ω–≤–∞—Ä—å 2025)
        self.extreme_agents = [
            # Chrome Windows (70% –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π)
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            
            # Firefox Windows (15% –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π)
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0',
            
            # Edge Windows (10% –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π)
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0',
            
            # Safari macOS (3% –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π)
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15'
        ]
    
    def _get_extreme_ua(self):
        """–ü–æ–ª—É—á–∞–µ—Ç —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π UA –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        # –í–∑–≤–µ—à–µ–Ω–Ω—ã–π –≤—ã–±–æ—Ä –ø–æ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏
        weights = [0.35, 0.35, 0.08, 0.07, 0.10, 0.05]  # Chrome –¥–æ–º–∏–Ω–∏—Ä—É–µ—Ç
        return random.choices(self.extreme_agents, weights=weights)[0]
    
    def _calculate_extreme_delay(self):
        """–≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –ø—Ä–æ—Ç–∏–≤ 429"""
        base_delay = self.extreme_delay_config['min_delay']
        
        # –ü—Ä–∏ –ª—é–±–æ–π –Ω–µ—É–¥–∞—á–µ - —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π —Ä–æ—Å—Ç
        if self.stats['consecutive_failures'] > 0:
            exponential_delay = min(
                self.extreme_delay_config['exponential_base'] ** self.stats['consecutive_failures'] * 300,
                self.extreme_delay_config['max_exponential']
            )
            base_delay = max(base_delay, exponential_delay)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ä–µ–º—è —Å—É—Ç–æ–∫ (–º–æ—Å–∫–æ–≤—Å–∫–æ–µ –≤—Ä–µ–º—è)
        current_hour = time.localtime().tm_hour
        
        # –ù–æ—á–Ω–æ–µ –≤—Ä–µ–º—è (00:00-06:00) - –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∏—Å–∫
        if 0 <= current_hour <= 6:
            base_delay *= 0.7
            print("üåô –ù–æ—á–Ω–æ–µ –≤—Ä–µ–º—è - —Å–Ω–∏–∂–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É")
            
        # –†–∞–±–æ—á–µ–µ –≤—Ä–µ–º—è (09:00-18:00) - –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∏—Å–∫
        elif 9 <= current_hour <= 18:
            base_delay *= 2.5
            print("üè¢ –†–∞–±–æ—á–µ–µ –≤—Ä–µ–º—è - —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É")
            
        # –í–µ—á–µ—Ä–Ω–µ–µ –≤—Ä–µ–º—è (18:00-23:00) - —Å—Ä–µ–¥–Ω–∏–π —Ä–∏—Å–∫
        elif 18 <= current_hour <= 23:
            base_delay *= 1.5
            print("üåÜ –í–µ—á–µ—Ä–Ω–µ–µ –≤—Ä–µ–º—è - —É–º–µ—Ä–µ–Ω–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞")
        
        # –°–ª—É—á–∞–π–Ω–∞—è –≤–∞—Ä–∏–∞—Ü–∏—è ¬±20%
        variation = random.uniform(0.8, 1.2)
        final_delay = base_delay * variation
        
        return min(final_delay, self.extreme_delay_config['max_delay'])
    
    def _wait_extreme_delay(self):
        """–≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        required_delay = self._calculate_extreme_delay()
        
        if time_since_last < required_delay:
            sleep_time = required_delay - time_since_last
            
            hours = int(sleep_time // 3600)
            minutes = int((sleep_time % 3600) // 60)
            seconds = int(sleep_time % 60)
            
            if hours > 0:
                print(f"ü•∑ –≠–ö–°–¢–†–ï–ú–ê–õ–¨–ù–ê–Ø –ó–ê–î–ï–†–ñ–ö–ê: {hours}—á {minutes}–º {seconds}—Å")
            elif minutes > 0:
                print(f"ü•∑ –≠–ö–°–¢–†–ï–ú–ê–õ–¨–ù–ê–Ø –ó–ê–î–ï–†–ñ–ö–ê: {minutes}–º {seconds}—Å")
            else:
                print(f"ü•∑ –≠–ö–°–¢–†–ï–ú–ê–õ–¨–ù–ê–Ø –ó–ê–î–ï–†–ñ–ö–ê: {seconds}—Å")
            
            print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {self.stats['successful_requests']}/{self.stats['total_requests']} —É—Å–ø–µ—à–Ω—ã—Ö")
            print(f"üíÄ –ù–µ—É–¥–∞—á –ø–æ–¥—Ä—è–¥: {self.stats['consecutive_failures']}")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 5 –º–∏–Ω—É—Ç
            self._show_extreme_progress(sleep_time)
        
        self.last_request_time = time.time()
    
    def _show_extreme_progress(self, total_seconds):
        """–ü–æ–∫–∞–∑ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ —Å –º–æ—Ç–∏–≤–∞—Ü–∏–µ–π"""
        print(f"ü•∑ –≠–ö–°–¢–†–ï–ú–ê–õ–¨–ù–û–ï –û–ñ–ò–î–ê–ù–ò–ï...")
        print("üò¥ –¢–µ—Ä–ø–µ–Ω–∏–µ - –Ω–∞—à–µ –≥–ª–∞–≤–Ω–æ–µ –æ—Ä—É–∂–∏–µ –ø—Ä–æ—Ç–∏–≤ 429!")
        
        motivational_quotes = [
            "üí™ –õ—É—á—à–µ –º–µ–¥–ª–µ–Ω–Ω–æ, –Ω–æ –≤–µ—Ä–Ω–æ!",
            "üéØ –ö–∞–∂–¥–∞—è —Å–µ–∫—É–Ω–¥–∞ –ø—Ä–∏–±–ª–∏–∂–∞–µ—Ç –∫ —É—Å–ø–µ—Ö—É!",
            "üöÄ –ê–≤–∏—Ç–æ –Ω–µ –æ–∂–∏–¥–∞–µ—Ç —Ç–∞–∫–æ–≥–æ —Ç–µ—Ä–ø–µ–Ω–∏—è!",
            "‚ö° –°—Ç–µ–ª—Å-—Ä–µ–∂–∏–º –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω –Ω–∞ –º–∞–∫—Å–∏–º—É–º!",
            "üé™ –ò–≥—Ä–∞–µ–º –≤ –¥–æ–ª–≥—É—é –∏–≥—Ä—É!",
            "üî• –ù–∞—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å - –Ω–∞—à —Å–µ–∫—Ä–µ—Ç!"
        ]
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 5 –º–∏–Ω—É—Ç –∏–ª–∏ –ø—Ä–∏ –∫–æ—Ä–æ—Ç–∫–∏—Ö –∑–∞–¥–µ—Ä–∂–∫–∞—Ö –∫–∞–∂–¥—ã–µ 30 —Å–µ–∫
        interval = 300 if total_seconds > 600 else 30
        
        for remaining in range(int(total_seconds), 0, -interval):
            if remaining > interval:
                hours = remaining // 3600
                minutes = (remaining % 3600) // 60
                
                if hours > 0:
                    print(f"   ‚è∞ –û—Å—Ç–∞–ª–æ—Å—å: {hours}—á {minutes}–º (–≤—Ä–µ–º—è: {time.strftime('%H:%M')})")
                else:
                    print(f"   ‚è∞ –û—Å—Ç–∞–ª–æ—Å—å: {minutes}–º (–≤—Ä–µ–º—è: {time.strftime('%H:%M')})")
                
                if remaining % (interval * 3) == 0:  # –ö–∞–∂–¥—ã–µ 15 –º–∏–Ω—É—Ç –º–æ—Ç–∏–≤–∞—Ü–∏—è
                    print(f"   {random.choice(motivational_quotes)}")
            
            time.sleep(min(interval, remaining))
        
        print("üéØ –ó–∞–¥–µ—Ä–∂–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –ü—Ä–∏—Å—Ç—É–ø–∞–µ–º –∫ —Å—Ç–µ–ª—Å-–∞—Ç–∞–∫–µ!")
    
    def extreme_parse_url(self, url: str) -> ParseResult:
        """–≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ –æ—Å—Ç–æ—Ä–æ–∂–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥"""
        print(f"\nü•∑ –≠–ö–°–¢–†–ï–ú–ê–õ–¨–ù–´–ô –°–¢–ï–õ–° –ø–∞—Ä—Å–∏–Ω–≥: {url}")
        
        if not self._is_avito_url(url):
            return ParseResult(url=url, error="–ù–µ–≤–µ—Ä–Ω—ã–π URL –ê–≤–∏—Ç–æ", success=False)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        self._wait_extreme_delay()
        
        start_time = time.time()
        
        # –¢–æ–ª—å–∫–æ —Å–∞–º—ã–π –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –º–µ—Ç–æ–¥
        print("ü•∑ –ú–µ—Ç–æ–¥: –¢–û–õ–¨–ö–û –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–π Requests")
        result = self._try_extreme_requests(url)
        
        result.response_time = time.time() - start_time
        
        if result.success:
            self._update_extreme_stats(True)
            print("üèÜ –≠–ö–°–¢–†–ï–ú–ê–õ–¨–ù–´–ô –£–°–ü–ï–•!")
        else:
            self._update_extreme_stats(False)
            print(f"üíÄ –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–∞—è –Ω–µ—É–¥–∞—á–∞: {result.error}")
        
        return result
    
    def _try_extreme_requests(self, url: str) -> ParseResult:
        """–°–∞–º—ã–π –æ—Å—Ç–æ—Ä–æ–∂–Ω—ã–π requests –∑–∞–ø—Ä–æ—Å"""
        try:
            # –°–æ–∑–¥–∞–µ–º —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
            session = requests.Session()
            
            # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
            headers = {
                'User-Agent': self._get_extreme_ua(),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
                'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1',
                'Cache-Control': 'no-cache',
                'Pragma': 'no-cache'
            }
            
            # –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ referer'—ã
            referers = [
                'https://www.google.com/search?q=–ø–æ–¥—É—à–∫–∞+–∫–æ—à–∞—á—å—è+–ª–∞–ø–∞+–∫—É–ø–∏—Ç—å',
                'https://yandex.ru/search/?text=–¥–µ—Ç—Å–∫–∏–µ+—Ç–æ–≤–∞—Ä—ã+–º–æ—Å–∫–≤–∞',
                'https://www.avito.ru/',
                'https://www.avito.ru/moskva'
            ]
            
            headers['Referer'] = random.choice(referers)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å–ª—É—á–∞–π–Ω–æ—Å—Ç—å
            if random.choice([True, False]):
                headers['sec-ch-ua'] = '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"'
                headers['sec-ch-ua-mobile'] = '?0'
                headers['sec-ch-ua-platform'] = '"Windows"'
            
            session.headers.update(headers)
            
            # –û—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
            print("üêå –í—ã–ø–æ–ª–Ω—è–µ–º –≠–ö–°–¢–†–ï–ú–ê–õ–¨–ù–û –º–µ–¥–ª–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å...")
            
            response = session.get(
                url, 
                timeout=60,  # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π —Ç–∞–π–º–∞—É—Ç
                allow_redirects=True,
                stream=False
            )
            
            response.encoding = 'utf-8'
            
            print(f"üìä –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–π Requests –æ—Ç–≤–µ—Ç: {response.status_code}")
            
            if response.status_code == 200:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
                if self._detect_extreme_blocking(response.text, response.headers):
                    return ParseResult(url=url, method="extreme_requests", error="–ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∞", success=False)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
                description, images = self._extract_content_from_html(response.content)
                
                if description or images:
                    return ParseResult(
                        url=url,
                        description=description,
                        images=images,
                        method="extreme_requests",
                        success=True
                    )
                else:
                    return ParseResult(url=url, method="extreme_requests", error="–ö–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω", success=False)
            
            elif response.status_code == 429:
                return ParseResult(url=url, method="extreme_requests", error="HTTP 429 - —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤", success=False)
            
            else:
                return ParseResult(url=url, method="extreme_requests", error=f"HTTP {response.status_code}", success=False)
            
        except requests.exceptions.RequestException as e:
            return ParseResult(url=url, method="extreme_requests", error=f"Request error: {e}", success=False)
        except Exception as e:
            return ParseResult(url=url, method="extreme_requests", error=str(e), success=False)
    
    def _detect_extreme_blocking(self, text: str, headers: dict = None) -> bool:
        """–≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–∞—è –¥–µ—Ç–µ–∫—Ü–∏—è –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫"""
        if not text:
            return True
            
        text_lower = text.lower()
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ 429 –∏ –¥—Ä—É–≥–∏—Ö –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
        blocking_indicators = [
            '429', 'too many requests', '—Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤',
            'rate limit', '–ø—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç', '—á–∞—Å—Ç–æ—Ç–∞ –∑–∞–ø—Ä–æ—Å–æ–≤',
            'captcha', '–∫–∞–ø—Ç—á–∞', 'robot', '–±–æ—Ç',
            'blocked', '–∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω', 'access denied',
            'security check', '–ø—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏',
            '–ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å', 'suspicious activity',
            '–≤—Ä–µ–º–µ–Ω–Ω–∞—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞', 'temporary block',
            'cloudflare', '–∑–∞—â–∏—Ç–∞ –æ—Ç ddos',
            '–∞–≤–∏—Ç–æ –∑–∞—â–∏—Ç–∞', 'avito protection'
        ]
        
        for indicator in blocking_indicators:
            if indicator in text_lower:
                print(f"üö® –≠–ö–°–¢–†–ï–ú–ê–õ–¨–ù–ê–Ø –ë–õ–û–ö–ò–†–û–í–ö–ê: {indicator}")
                return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –æ—Ç–≤–µ—Ç–∞
        if headers:
            # Cloudflare –∑–∞–≥–æ–ª–æ–≤–∫–∏
            cf_headers = ['cf-ray', 'cf-cache-status', 'server']
            cf_found = sum(1 for header in cf_headers if header.lower() in [k.lower() for k in headers.keys()])
            
            if cf_found >= 2:
                print("üö® Cloudflare –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω")
                return True
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º server –∑–∞–≥–æ–ª–æ–≤–æ–∫
            server = headers.get('Server', '').lower()
            if any(suspicious in server for suspicious in ['cloudflare', 'ddos-guard']):
                print(f"üö® –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–π —Å–µ—Ä–≤–µ—Ä: {server}")
                return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É –æ—Ç–≤–µ—Ç–∞
        if len(text) < 5000:
            print(f"üö® –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ê–≤–∏—Ç–æ
        essential_elements = ['avito', 'item', 'description', 'price']
        found_elements = sum(1 for element in essential_elements if element in text_lower)
        
        if found_elements < 2:
            print(f"üö® –ù–µ –Ω–∞–π–¥–µ–Ω—ã –∫–ª—é—á–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã: {found_elements}/{len(essential_elements)}")
            return True
        
        return False
    
    def _extract_content_from_html(self, html_content, is_mobile: bool = False):
        """–≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        try:
            if isinstance(html_content, bytes):
                try:
                    import chardet
                    detected = chardet.detect(html_content)
                    encoding = detected.get('encoding', 'utf-8')
                    html_content = html_content.decode(encoding, errors='replace')
                except:
                    html_content = html_content.decode('utf-8', errors='replace')
            
            soup = BeautifulSoup(html_content, 'html.parser')
            
            description = self._extract_description_extreme(soup)
            images = self._extract_images_extreme(soup)
            
            return description, images
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {e}")
            return None, []
    
    def _extract_description_extreme(self, soup):
        """–≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è"""
        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —à–∏—Ä–æ–∫–∏–π —Å–ø–∏—Å–æ–∫ —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤
        selectors = [
            # –û—Å–Ω–æ–≤–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            '[data-marker="item-view/item-description"]',
            '[data-marker*="description"]',
            '.item-description-text',
            '[itemprop="description"]',
            '.item-description',
            '.item-description-content',
            '.item-view-description',
            '.item-text',
            '.js-item-description',
            
            # Meta —Ç–µ–≥–∏
            'meta[name="description"]',
            'meta[property="og:description"]',
            'meta[name="twitter:description"]',
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            '.iva-item-text',
            '.description',
            '.text',
            '.content',
            '.item-body',
            '.item-content',
            
            # JSON-LD –¥–∞–Ω–Ω—ã–µ
            'script[type="application/ld+json"]'
        ]
        
        for selector in selectors:
            try:
                if selector.startswith('meta'):
                    element = soup.select_one(selector)
                    if element:
                        content = element.get('content', '').strip()
                        if len(content) > 20:
                            print(f"üìÑ –û–ø–∏—Å–∞–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–æ ({selector}): {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
                            return content
                
                elif 'script' in selector:
                    # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å –∏–∑ JSON-LD
                    scripts = soup.select(selector)
                    for script in scripts:
                        try:
                            data = json.loads(script.string or '')
                            if isinstance(data, dict) and 'description' in data:
                                desc = data['description'].strip()
                                if len(desc) > 20:
                                    print(f"üìÑ –û–ø–∏—Å–∞–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–æ (JSON-LD): {len(desc)} —Å–∏–º–≤–æ–ª–æ–≤")
                                    return desc
                        except:
                            continue
                
                else:
                    element = soup.select_one(selector)
                    if element:
                        text = element.get_text(separator=' ', strip=True)
                        if len(text) > 20:
                            print(f"üìÑ –û–ø–∏—Å–∞–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–æ ({selector}): {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
                            return text
                            
            except Exception as e:
                continue
        
        print("‚ö†Ô∏è –û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        return None
    
    def _extract_images_extreme(self, soup):
        """–≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        images = []
        
        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —à–∏—Ä–æ–∫–∏–π —Å–ø–∏—Å–æ–∫ —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        selectors = [
            '[data-marker="item-view/gallery"] img',
            '[data-marker*="gallery"] img',
            '.gallery-img-frame img',
            '.item-photo img',
            '.item-view img',
            'img[src*="avito"]',
            'img[data-src*="avito"]',
            '.iva-item-photo img',
            '.gallery img',
            '.js-gallery img',
            '.item-gallery img',
            '.photo img',
            'img[alt*="—Ñ–æ—Ç–æ"]',
            'img[alt*="–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"]'
        ]
        
        for selector in selectors:
            try:
                img_elements = soup.select(selector)
                for img in img_elements:
                    # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã
                    src_attrs = ['src', 'data-src', 'data-lazy', 'data-original', 'data-srcset']
                    
                    for attr in src_attrs:
                        src = img.get(attr)
                        if src and src not in images:
                            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º URL
                            if src.startswith('//'):
                                src = 'https:' + src
                            elif src.startswith('/'):
                                src = 'https://www.avito.ru' + src
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
                            if any(ext in src.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                                # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                                if not any(skip in src.lower() for skip in [
                                    'avatar', 'logo', 'icon', 'sprite', 'button', 
                                    'placeholder', 'default', 'no-image'
                                ]):
                                    images.append(src)
                                    
            except Exception as e:
                continue
        
        print(f"üñºÔ∏è –ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(images)}")
        return images
    
    def _update_extreme_stats(self, success: bool):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        self.stats['total_requests'] += 1
        
        if success:
            self.stats['successful_requests'] += 1
            self.stats['last_success_time'] = time.time()
            self.stats['consecutive_failures'] = 0
        else:
            self.stats['consecutive_failures'] += 1
            
            # –ü—Ä–∏ 429 —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
            self.stats['blocked_requests'] += 1
    
    def _is_avito_url(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç URL –ê–≤–∏—Ç–æ"""
        return 'avito.ru' in urlparse(url).netloc


def extreme_safe_parse_item(url: str) -> Dict[str, Any]:
    """–≠–ö–°–¢–†–ï–ú–ê–õ–¨–ù–û –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥"""
    parser = None
    try:
        parser = ExtremeStealthAvitoParser()
        result = parser.extreme_parse_url(url)
        
        return {
            'url': result.url,
            'description': result.description,
            'images': result.images,
            'method': result.method,
            'success': result.success,
            'error': result.error,
            'response_time': result.response_time,
            'stats': parser.stats
        }
        
    except Exception as e:
        return {
            'url': url,
            'description': None,
            'images': [],
            'method': 'error',
            'success': False,
            'error': str(e),
            'response_time': 0.0,
            'stats': {}
        }
    finally:
        if parser:
            print("üîí –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–π Parser –∑–∞–∫—Ä—ã—Ç")


if __name__ == '__main__':
    print("üì¶ –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏...")
    
    required_packages = {
        'requests': 'requests',
        'cloudscraper': 'cloudscraper',
        'httpx': 'httpx',
        'fake_useragent': 'fake-useragent',
        'bs4': 'beautifulsoup4'
    }
    
    missing_packages = []
    for package, install_name in required_packages.items():
        try:
            __import__(package)
            print(f"‚úÖ {package} –¥–æ—Å—Ç—É–ø–µ–Ω")
        except ImportError:
            print(f"‚ùå {package} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            missing_packages.append(install_name)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º chardet –¥–ª—è –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–æ–¥–∏—Ä–æ–≤–∫–∏
    try:
        import chardet
        print("‚úÖ chardet –¥–æ—Å—Ç—É–ø–µ–Ω")
    except ImportError:
        print("‚ö†Ô∏è chardet –Ω–µ –Ω–∞–π–¥–µ–Ω (–±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ UTF-8 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)")
        print("üí° –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å: pip install chardet")
    
    if missing_packages:
        print(f"\nüí° –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –ø–∞–∫–µ—Ç—ã:")
        print(f"pip install {' '.join(missing_packages)}")
        exit(1)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ curl
    try:
        subprocess.run(['curl', '--version'], capture_output=True, timeout=5)
        print("‚úÖ curl –¥–æ—Å—Ç—É–ø–µ–Ω")
    except:
        print("‚ö†Ô∏è curl –Ω–µ –Ω–∞–π–¥–µ–Ω (–º–µ—Ç–æ–¥ curl –±—É–¥–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω)")
    
    test_urls = [
        "https://www.avito.ru/moskva/tovary_dlya_detey_i_igrushki/myagkaya_podushka_koshachya_lapa_7531657296",
    ]
    
    print(f"\nüöÄ –ü–†–û–î–í–ò–ù–£–¢–´–ô –ú–£–õ–¨–¢–ò–ú–ï–¢–û–î–ù–´–ô –†–ï–ñ–ò–ú")
    print(f"{'='*80}")
    print("‚ú® 6 –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –º–µ—Ç–æ–¥–æ–≤ —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫–∏!")
    print("üõ°Ô∏è –£–ª—É—á—à–µ–Ω–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ —Å—Ç–µ–ª—Å-—Ä–µ–∂–∏–º—ã!")
    print("üì± –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–±–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è!")
    print("üîß CURL —Å HTTP/2 –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π!")
    print("üîç –ü–æ–∏—Å–∫ API —ç–Ω–¥–ø–æ–∏–Ω—Ç–æ–≤!")
    print("üìä –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏!")
    print(f"{'='*80}")
    
    for i, url in enumerate(test_urls, 1):
        print(f"\nüéØ –ü–†–û–î–í–ò–ù–£–¢–´–ô –ú–£–õ–¨–¢–ò–ú–ï–¢–û–î–ù–´–ô –¢–ï–°–¢ {i}: {url[:60]}...")
        
        result = safe_parse_item(url)
        
        print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢:")
        print(f"   ‚úÖ –£—Å–ø–µ—Ö: {result['success']}")
        print(f"   üîß –£—Å–ø–µ—à–Ω—ã–π –º–µ—Ç–æ–¥: {result['method']}")
        print(f"   ‚è±Ô∏è –í—Ä–µ–º—è: {result['response_time']:.1f}—Å")
        print(f"   üìÑ –û–ø–∏—Å–∞–Ω–∏–µ: {'–î–∞' if result['description'] else '–ù–µ—Ç'}")
        print(f"   üñºÔ∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(result['images'])}")
        if result['error']:
            print(f"   ‚ùå –û—à–∏–±–∫–∞: {result['error']}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –º–µ—Ç–æ–¥–æ–≤
        if result['stats']:
            print(f"\nüìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ú–ï–¢–û–î–û–í:")
            for method, stats in result['stats'].items():
                print(f"   {method}: {stats['success_rate']} ({stats['successful']}/{stats['total']})")
        
        if result['description']:
            print(f"\nüìÑ –û–ü–ò–°–ê–ù–ò–ï: {result['description'][:200]}...")
        
        if result['images']:
            print(f"\nüñºÔ∏è –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–Ø:")
            for j, img in enumerate(result['images'][:5], 1):
                print(f"   {j}. {img}")
    
    print(f"\nüèÅ –ü–†–û–î–í–ò–ù–£–¢–´–ô –ú–£–õ–¨–¢–ò–ú–ï–¢–û–î–ù–´–ô —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print("üí° –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –æ–±—Ö–æ–¥ –∑–∞—â–∏—Ç—ã!")

    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ—Å—Ç —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞
    print(f"\n\nü•∑ –≠–ö–°–¢–†–ï–ú–ê–õ–¨–ù–´–ô –°–¢–ï–õ–° –†–ï–ñ–ò–ú")
    print(f"{'='*80}")
    print("‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ú–∏–Ω–∏–º—É–º 5 –º–∏–Ω—É—Ç –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏!")
    print("‚ö†Ô∏è  –ü—Ä–∏ –Ω–µ—É–¥–∞—á–µ 429 - –ø–∞—É–∑–∞ –¥–æ 2 —á–∞—Å–æ–≤!")
    print("‚ö†Ô∏è  –¢–æ–ª—å–∫–æ 1 –∑–∞–ø—Ä–æ—Å –∑–∞ —Å–µ—Å—Å–∏—é!")
    print("‚ö†Ô∏è  –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∑–∞–ø—É—Å–∫–∞—Ç—å —Ç–æ–ª—å–∫–æ –Ω–æ—á—å—é!")
    print(f"{'='*80}")
    
    current_hour = time.localtime().tm_hour
    if 9 <= current_hour <= 18:
        print("üö® –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –†–∞–±–æ—á–µ–µ –≤—Ä–µ–º—è - –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏!")
        user_input = input("ü§î –í—Å–µ —Ä–∞–≤–Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å? (yes/no): ").lower()
        if user_input not in ['yes', 'y', '–¥–∞']:
            print("üåô –ú—É–¥—Ä–æ–µ —Ä–µ—à–µ–Ω–∏–µ! –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –Ω–æ—á—å—é (00:00-06:00).")
            exit(0)
    
    for i, url in enumerate(test_urls, 1):
        print(f"\nü•∑ –≠–ö–°–¢–†–ï–ú–ê–õ–¨–ù–´–ô –°–¢–ï–õ–° –¢–ï–°–¢ {i}: {url[:60]}...")
        
        result = extreme_safe_parse_item(url)
        
        print(f"\nüìä –≠–ö–°–¢–†–ï–ú–ê–õ–¨–ù–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢:")
        print(f"   ‚úÖ –£—Å–ø–µ—Ö: {result['success']}")
        print(f"   üîß –ú–µ—Ç–æ–¥: {result['method']}")
        print(f"   ‚è±Ô∏è –í—Ä–µ–º—è: {result['response_time']:.1f}—Å")
        print(f"   üìÑ –û–ø–∏—Å–∞–Ω–∏–µ: {'–î–∞' if result['description'] else '–ù–µ—Ç'}")
        print(f"   üñºÔ∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(result['images'])}")
        if result['error']:
            print(f"   ‚ùå –û—à–∏–±–∫–∞: {result['error']}")
        
        if result['description']:
            print(f"\nüìÑ –û–ü–ò–°–ê–ù–ò–ï: {result['description'][:200]}...")
        
        if result['images']:
            print(f"\nüñºÔ∏è –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–Ø:")
            for j, img in enumerate(result['images'][:3], 1):
                print(f"   {j}. {img}")
        
        # –í–ê–ñ–ù–û: —Ç–æ–ª—å–∫–æ 1 –∑–∞–ø—Ä–æ—Å –≤ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ–º —Ä–µ–∂–∏–º–µ!
        print("\nüõë –≠–ö–°–¢–†–ï–ú–ê–õ–¨–ù–´–ô –†–ï–ñ–ò–ú: —Ç–æ–ª—å–∫–æ 1 –∑–∞–ø—Ä–æ—Å –∑–∞ —Å–µ—Å—Å–∏—é!")
        break
    
    print(f"\nüèÅ –≠–ö–°–¢–†–ï–ú–ê–õ–¨–ù–´–ô –°–¢–ï–õ–° —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print("üí° –ü–æ–º–Ω–∏: —Ç–µ—Ä–ø–µ–Ω–∏–µ –∏ –≤—Ä–µ–º—è - –Ω–∞—à–∏ –≥–ª–∞–≤–Ω—ã–µ —Å–æ—é–∑–Ω–∏–∫–∏ –ø—Ä–æ—Ç–∏–≤ 429!")